{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46034a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create dataset\n",
    "## - load dataset.tsv (output from prepare_tts_data.dart)\n",
    "## - convert wav to w2v features\n",
    "## - third column is text IDs (but this is from text_to_ipa.dart which is just using the lookup, should we switch to gruut?)\n",
    "# input 1 is w2v features (audio_len, w2v_dim)\n",
    "# input 2 is txt indices (text_len)\n",
    "\n",
    "# Basic model \n",
    "# - phones (text) -> embedding (Fp) ---\\  (text = k/q, audio = v)\n",
    "#                              -> transformer (O) ------------------------>\n",
    "# - audio -> w2v (Fw) --------/\n",
    "#             \\\n",
    "#              \\-----> randomly select p indices (R)\n",
    "#                                \\ \n",
    "#                                 \\------------------> mask M steps with feature vector (Fw_m)\n",
    "#\n",
    "# - gather transformer output at R\n",
    "# - sample K candidates from Fw(R) and the actual candidate foreach p Fw\n",
    "# - at each step t, calculate similarity Ot * Fw(R)\n",
    "# - softmax\n",
    "# - sum loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a06b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "from fairseq import checkpoint_utils\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Model\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import soundfile as sf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0193393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedWav2VecModel(nn.Module):\n",
    "    def __init__(self, fname):\n",
    "        super().__init__()\n",
    "\n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([fname])\n",
    "        model = model[0]\n",
    "        model.eval()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.model.feature_extractor(x)\n",
    "            if isinstance(z, tuple):\n",
    "                z = z[0]\n",
    "            #c = self.model.feature_aggregator(z)\n",
    "        return z.to(device)\n",
    "    \n",
    "w2v = PretrainedWav2VecModel(\"/mnt/sdd_512gb/models/xlsr_53_56k.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d58fc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(audio, sample_rate, pad_len_in_secs):\n",
    "    pad_len_in_samples = pad_len_in_secs * sample_rate\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio[0]\n",
    "    padded = 0\n",
    "    if audio.shape[0] < pad_len_in_samples:\n",
    "        padded = (pad_len_in_secs * sample_rate) - audio.shape[0]\n",
    "        audio = np.pad(audio, (0, padded), constant_values=0.000)\n",
    "    elif audio.shape[0] > pad_len_in_samples:\n",
    "        audio = audio[:pad_len_in_samples]\n",
    "    return audio, padded\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, transcript_file, audio_pad_to=6, transcript_pad_to=40):\n",
    "        self.audio_files = []\n",
    "        self.transcripts = []\n",
    "        self.audio_pad_to = audio_pad_to\n",
    "        self.transcript_pad_to = transcript_pad_to\n",
    "        with open(transcript_file, \"r\") as infile:\n",
    "            for line in infile.readlines():\n",
    "                split = line.strip().split(\"\\t\")\n",
    "                self.audio_files.append(split[0])\n",
    "                self.transcripts.append([int(symbol_id) for symbol_id in split[1].split(\" \")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = sf.read(self.audio_files[idx], dtype=np.float32)\n",
    "        wav, _ = pad_audio(wav, sr, self.audio_pad_to)\n",
    "        return wav, torch.LongTensor(self.transcripts[idx])\n",
    "\n",
    "dataset = AudioDataset(\"/tmp/nick_phonemes/transcripts.tsv\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=36, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acba76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_select_and_reshape(inp, mask):\n",
    "    masked = torch.masked_select(inp, torch.unsqueeze(mask,dim=1))        \n",
    "    masked = torch.reshape(masked, (masked.size()[0] // inp.size()[1], inp.size()[1]))\n",
    "    return masked\n",
    "\n",
    "# \n",
    "# Replaces samples in [inp] with [replacement] along [dim] with probability [p].\n",
    "# If an index [i] in [inp] is chosen for replacement, [m] samples will be replaced\n",
    "# In pseudo-code:\n",
    "# for i in len(inp):\n",
    "#    if rand() > p:\n",
    "#        inp[i:i+m] = replacement\n",
    "#        i += m\n",
    "# Returns a 3-tuple of:\n",
    "# - the input tensor, after replacing the masked indices with the replacement vector\n",
    "# - the original values in the input tensor that were replaced with the replacement vector (i.e. excluding any vectors that were not replaced)\n",
    "# - the boolean mask \n",
    "def sample_and_replace(inp, replacement, p=0.5, m=8, dim=0):\n",
    "    # randomly select the starting indices to replace\n",
    "    index_mask  = torch.randn(inp.size()[dim]).ge(p) \n",
    "    # expand each index to cover i:i+m\n",
    "    for i in range(index_mask.size()[0]):\n",
    "        if index_mask[i] is True:\n",
    "            index_mask[i:i+m] = True\n",
    "            i += m\n",
    "        \n",
    "    masked_feats = torch.masked_select(inp, torch.unsqueeze(index_mask,dim=1).to(device))        \n",
    "    masked_feats = torch.reshape(masked_feats, (masked_feats.size()[0] // inp.size()[1], inp.size()[1]))\n",
    "    \n",
    "    feats = inp.clone()\n",
    "    feats[index_mask] = replacement    \n",
    "    return feats, masked_feats, index_mask\n",
    "\n",
    "#\n",
    "# For each entry in [inp] (where [inp] is (b,n,d)), sample (k-1) entries from [inp]\n",
    "# Returns a tensor [result] of size (b,n,k,d)\n",
    "# where result[t,0,:] is the original value at inp[t,:]\n",
    "# and result[t,k:,:] are the (k-1) sampled values (where samples have been drawn from n excluding t)\n",
    "#\n",
    "def sample_k_candidates(inp, k):\n",
    "    \n",
    "    candidates_b = torch.zeros((inp.size()[0], inp.size()[1], k+1, inp.size()[2]))\n",
    "    \n",
    "    for b in range(inp.size()[0]):\n",
    "        for t in range(inp.size()[1]):\n",
    "            indices = torch.LongTensor(np.random.choice([i for i in np.arange(inp.size()[1]) if i != t], k+1, replace=False))\n",
    "            candidates_t = torch.index_select(inp[b], 0, indices) # this will be (k+1,d)            \n",
    "            candidates_t[0] = inp[b,t] # ensure the true value is at index 0\n",
    "            candidates_b[b, t, ] = candidates_t\n",
    "    return candidates_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efde4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSP(nn.Module):\n",
    "    def __init__(self, num_phones=100, audio_dim=1024, num_heads=4, replace=0.5, k=50):\n",
    "        super().__init__()\n",
    "        self.replace = 0.5\n",
    "        self.phone_embedding = nn.Embedding(num_phones, audio_dim)\n",
    "        self.attention = nn.MultiheadAttention(audio_dim, num_heads)\n",
    "        self.replacement = Variable(torch.rand(1, audio_dim), requires_grad=True)\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, phones, audio):\n",
    "        \n",
    "        audio_feats, masked_audio_feats, audio_mask = sample_and_replace(audio, self.replacement, self.replace)\n",
    "        \n",
    "        phone_feats = self.phone_embedding(x)\n",
    "        \n",
    "        o, _ = self.attention(phone_feats, phone_feats, masked_audio_feats)\n",
    "        \n",
    "        # gather the output entries at the masked indices\n",
    "        masked_output = masked_select_and_reshape(o, audio_mask)\n",
    "        \n",
    "        # sample K entries from the (masked) audio input entries\n",
    "        candidates, candidate_indices = sample_k_candidates(masked_audio_feats, self.k)\n",
    "                \n",
    "        return masked_output, candidates, candidate_indices\n",
    "\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=3, eps=1e-6)\n",
    "\n",
    "model = SSP()\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8940f4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mask in method wrapper_masked_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2386240/1605102883.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maudio_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmasked_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# masked_output is (b,n,d) and candidates is (b,n,k,d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2386240/846832595.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, phones, audio)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maudio_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_audio_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_and_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mphone_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphone_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2386240/698791483.py\u001b[0m in \u001b[0;36msample_and_replace\u001b[0;34m(inp, replacement, p, m, dim)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mmasked_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmasked_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasked_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument mask in method wrapper_masked_select)"
     ]
    }
   ],
   "source": [
    "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "steps = 10000\n",
    "w2v.eval()\n",
    "for s in range(steps):\n",
    "    audio, phones = next(iter(train_dataloader))\n",
    "    audio = torch.FloatTensor(audio).to(device)\n",
    "    audio_feats = w2v.forward(audio)\n",
    "\n",
    "    masked_output, candidates, candidate_indices = model.forward(phones.to(device), audio_feats.to(device))\n",
    "\n",
    "    # masked_output is (b,n,d) and candidates is (b,n,k,d)\n",
    "    # expand masked_output to (b,n,k,d)\n",
    "    masked_output = masked_output.expand(candidates.size())\n",
    "    similarity = torch.exp(cos(masked_output, candidates) / k)\n",
    "    loss = torch.sum(similarity[:,:,0] / torch.sum(sim,dim=2))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab704f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,2,5)\n",
    "b = torch.rand(1,2,5,3)\n",
    "print(a)\n",
    "print(b)\n",
    "torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.array([0.8291, 0.5697, 0.3417, 0.6862, 0.0465]), np.array([0.2992, 0.0023, 0.4620, 0.0736, 0.9372]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0,0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7525a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,4,1,5)\n",
    "b = torch.rand(1,4,7,5)\n",
    "cos = nn.CosineSimilarity(dim=3, eps=1e-6)\n",
    "#for i in range(b.size()[2]):\n",
    "#    print(cos(a, b[:,:,i,:]))\n",
    "    \n",
    "a.expand(b.size())\n",
    "sim = torch.exp(cos(a,b) / 5)\n",
    "print(sim.size())\n",
    "print(sim[:,:,0].size())\n",
    "print(torch.sum(sim, dim=2).size())\n",
    "sim[:,:,0] / torch.sum(sim,dim=2)\n",
    "\n",
    "\n",
    "   \n",
    "#inp = torch.randn((5, 1000, 1024))\n",
    "# v = torch.ones((1,1024))\n",
    "# _, _, m = sample_and_replace(inp, v)\n",
    "# print(m[m == True].size())\n",
    "# masked_select_and_reshape(inp, m).size()\n",
    "#sample_k_candidates(inp, 5)\n",
    "\n",
    "#sample_k_candidates(inp, 3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
