{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46034a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create dataset\n",
    "## - load dataset.tsv (output from prepare_tts_data.dart)\n",
    "## - convert wav to w2v features\n",
    "## - third column is text IDs (but this is from text_to_ipa.dart which is just using the lookup, should we switch to gruut?)\n",
    "# input 1 is w2v features (audio_len, w2v_dim)\n",
    "# input 2 is txt indices (text_len)\n",
    "\n",
    "# Basic model \n",
    "# - phones (text) -> embedding (Fp) ---\\  (text = k/q, audio = v)\n",
    "#                              -> transformer (O) ------------------------>\n",
    "# - audio -> w2v (Fw) --------/\n",
    "#             \\\n",
    "#              \\-----> randomly select p indices (R)\n",
    "#                                \\ \n",
    "#                                 \\------------------> mask M steps with feature vector (Fw_m)\n",
    "#\n",
    "# - gather transformer output at R\n",
    "# - sample K candidates from Fw(R) and the actual candidate foreach p Fw\n",
    "# - at each step t, calculate similarity Ot * Fw(R)\n",
    "# - softmax\n",
    "# - sum loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a06b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 10:48:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "from fairseq import checkpoint_utils\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Model\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import soundfile as sf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b91d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedWav2VecModel(nn.Module):\n",
    "    def __init__(self, fname):\n",
    "        super().__init__()\n",
    "\n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([fname])\n",
    "        model = model[0]\n",
    "        model.eval()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.model.feature_extractor(x)\n",
    "            if isinstance(z, tuple):\n",
    "                z = z[0]\n",
    "            #c = self.model.feature_aggregator(z)\n",
    "        return z.to(device)\n",
    "    \n",
    "w2v = PretrainedWav2VecModel(\"/mnt/sdd_512gb/models/xlsr_53_56k.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9225c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(audio, sample_rate, pad_len_in_secs):\n",
    "    pad_len_in_samples = pad_len_in_secs * sample_rate\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio[0]\n",
    "    padded = 0\n",
    "    if audio.shape[0] < pad_len_in_samples:\n",
    "        padded = (pad_len_in_secs * sample_rate) - audio.shape[0]\n",
    "        audio = np.pad(audio, (0, padded), constant_values=0.000)\n",
    "    elif audio.shape[0] > pad_len_in_samples:\n",
    "        audio = audio[:pad_len_in_samples]\n",
    "    return audio, padded\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, transcript_file, audio_pad_to=6, transcript_pad_to=40):\n",
    "        self.audio_files = []\n",
    "        self.transcripts = []\n",
    "        self.audio_pad_to = audio_pad_to\n",
    "        self.transcript_pad_to = transcript_pad_to\n",
    "        with open(transcript_file, \"r\") as infile:\n",
    "            for line in infile.readlines():\n",
    "                split = line.strip().split(\"\\t\")\n",
    "                self.audio_files.append(split[0])\n",
    "                self.transcripts.append([int(symbol_id) for symbol_id in split[1].split(\" \")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = sf.read(self.audio_files[idx], dtype=np.float32)\n",
    "        wav, _ = pad_audio(wav, sr, self.audio_pad_to)\n",
    "        return wav, torch.LongTensor(self.transcripts[idx])\n",
    "\n",
    "dataset = AudioDataset(\"/tmp/nick_phonemes/transcripts.tsv\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee0ea5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# For each entry in [inp] (where [inp] is (b,n,d)), sample (k-1) entries from [inp]\n",
    "# Returns a tensor [result] of size (b,n,k,d)\n",
    "# where result[t,0,:] is the original value at inp[t,:]\n",
    "# and result[t,k:,:] are the (k-1) sampled values (where samples have been drawn from n excluding t)\n",
    "#\n",
    "def sample_k_candidates(inp, k):\n",
    "    \n",
    "    candidates_b = torch.zeros((inp.size()[0], inp.size()[1], k+1, inp.size()[2]))\n",
    "    \n",
    "    for b in range(inp.size()[0]):\n",
    "        for t in range(inp.size()[1]):\n",
    "            indices = torch.LongTensor(np.random.choice([i for i in np.arange(inp.size()[1]) if i != t], k+1, replace=False)).to(inp.device)\n",
    "            candidates_t = torch.index_select(inp[b], 0, indices) # this will be (k+1,d)            \n",
    "            candidates_t[0] = inp[b,t] # ensure the true value is at index 0\n",
    "            candidates_b[b, t, ] = candidates_t\n",
    "    return candidates_b.to(inp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae6f8942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4507,  0.7537, -0.1438, -0.3563, -0.1628],\n",
       "         [-0.6748, -0.9614, -0.1198,  0.0897,  1.9340]],\n",
       "\n",
       "        [[-0.2873, -0.8932,  0.5234, -0.9421, -0.8743],\n",
       "         [ 0.0362,  0.4017,  1.5341, -0.5441,  2.2342]],\n",
       "\n",
       "        [[-0.8734,  0.1844, -0.3851, -0.2396, -1.3658],\n",
       "         [-0.6179,  0.0268, -0.5751,  0.9189, -0.0922]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# Helper function to perform a reshape after a masked_select\n",
    "# Accepts [input] and [mask] both of size (b,n,d)\n",
    "# returns [b,?,d]\n",
    "def masked_select_and_reshape(inp, mask):\n",
    "    batch_size, seq_length,feat_dim = inp.size()\n",
    "    masked_feats = torch.masked_select(inp, mask)   \n",
    "    masked_seq_len = masked_feats.size()[0] // feat_dim // batch_size\n",
    "    masked_feats = torch.reshape(masked_feats, (batch_size, masked_seq_len, feat_dim))\n",
    "    return masked_feats\n",
    "\n",
    "# \n",
    "# Replaces samples in [inp] with [replacement] along [dim] with probability [p].\n",
    "# If an index [i] in [inp] is chosen for replacement, [m] samples will be replaced\n",
    "# In pseudo-code:\n",
    "# for i in len(inp):\n",
    "#    if rand() > p:\n",
    "#        inp[i:i+m] = replacement\n",
    "#        i += m\n",
    "# Returns a 3-tuple of:\n",
    "# - the input tensor, after replacing the masked indices with the replacement vector\n",
    "# - the original values in the input tensor that were replaced with the replacement vector (i.e. excluding any vectors that were not replaced)\n",
    "# - the boolean mask \n",
    "def sample_and_replace(inp, replacement, p=0.5, m=8):\n",
    "    batch_size, seq_length,feat_dim = inp.size()\n",
    "    # randomly select the starting indices to replace\n",
    "    # size (b,n)\n",
    "    # for convenience, re-use the same indices across the whole batch\n",
    "    index_mask  = torch.randn((1,seq_length)).ge(p) \n",
    "\n",
    "    # expand each index to cover i:i+m\n",
    "    for i in range(seq_length):\n",
    "        if index_mask[0,i] is True:\n",
    "            index_mask[0,i:i+m] = True\n",
    "            i += m\n",
    "    \n",
    "    index_mask_x = torch.unsqueeze(index_mask, dim=2).expand(inp.size()).to(inp.device)\n",
    "    masked_feats = masked_select_and_reshape(inp, index_mask_x).to(inp.device)\n",
    "\n",
    "    feats = inp.clone()\n",
    "    feats[:,index_mask[0,:],:] = replacement\n",
    "    return feats, masked_feats, index_mask_x\n",
    "   \n",
    "inp = torch.randn((3, 4, 5))\n",
    "v = torch.ones((1,5))\n",
    "k,l,m = sample_and_replace(inp, v)\n",
    "# print(m[m == True].size())\n",
    "#m = inp.ge(0.5)\n",
    "    #masked_feats = torch.masked_select(inp, index_mask_x)   \n",
    "    #masked_seq_len = masked_feats.size()[0] // feat_dim // batch_size\n",
    "    #masked_feats = torch.reshape(masked_feats, (batch_size, masked_seq_len, feat_dim))\n",
    "#sample_k_candidates(inp, 5)\n",
    "\n",
    "#sample_k_candidates(inp, 3)\n",
    "#x\n",
    "masked_select_and_reshape(inp, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde4931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1397.3331753015518\n",
      "Loss: 1363.529533147812\n",
      "Loss: 1370.019433259964\n",
      "Loss: 1367.0784862041473\n",
      "Loss: 1357.823569059372\n",
      "Loss: 1374.7842738628387\n",
      "Loss: 1344.1176441907883\n",
      "Loss: 1346.2549695968628\n"
     ]
    }
   ],
   "source": [
    "class SSP(nn.Module):\n",
    "    def __init__(self, num_phones=171, audio_dim=512, num_heads=4, replace=0.5, k=50):\n",
    "        super().__init__()\n",
    "        self.replace = 0.5\n",
    "        self.phone_embedding = nn.Embedding(num_phones, audio_dim)\n",
    "        self.attention = nn.MultiheadAttention(audio_dim, num_heads)\n",
    "        self.replacement = Variable(torch.rand(1, audio_dim), requires_grad=True).to(device)\n",
    "        self.k = k\n",
    "        \n",
    "    def forward(self, phones, audio):\n",
    "        phone_feats = self.phone_embedding(phones)\n",
    "        \n",
    "        audio_feats, masked_audio_feats, audio_mask = sample_and_replace(audio, self.replacement, self.replace)\n",
    "        \n",
    "        o, _ = self.attention(phone_feats, phone_feats, audio_feats)       \n",
    "        \n",
    "        # gather the output entries at the masked indices\n",
    "        masked_output = masked_select_and_reshape(o, audio_mask)\n",
    "        \n",
    "        # sample K+1 entries from the (masked) audio input entries\n",
    "        # entry at index 0 is the true frame\n",
    "        candidates = sample_k_candidates(masked_audio_feats, self.k)\n",
    "                \n",
    "        return masked_output, candidates\n",
    "\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=3, eps=1e-6)\n",
    "\n",
    "model = SSP()\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "steps = 10000\n",
    "print_loss_every = 50\n",
    "accum_loss = 0\n",
    "w2v.eval()\n",
    "batch = iter(train_dataloader)\n",
    "for s in range(steps):\n",
    "    \n",
    "    audio, phones = next(batch, (None, None))\n",
    "    if audio is None:\n",
    "        batch = iter(train_dataloader)\n",
    "        audio, phones = next(batch, (None, None))\n",
    "        \n",
    "    audio = torch.FloatTensor(audio).to(device)\n",
    "    audio_feats = w2v.forward(audio)\n",
    "    audio_feats = torch.transpose(audio_feats, 1,2).to(device)\n",
    "\n",
    "    masked_output, candidates = model.forward(phones.to(device), audio_feats.to(device))\n",
    "\n",
    "    # masked_output is (b,n,d) and candidates is (b,n,k,d)\n",
    "    # expand masked_output to (b,n,k,d)    \n",
    "    masked_output = torch.unsqueeze(masked_output, dim=2).expand(candidates.size()).to(device)\n",
    "    unexp = cos(masked_output, candidates) / model.k\n",
    "    similarity = torch.exp(unexp)\n",
    "    \n",
    "    loss = torch.sum(similarity[:,:,0] / torch.sum(similarity,dim=2))\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    accum_loss += loss.item()\n",
    "    if s > 0 and s % print_loss_every == 0:\n",
    "        print(f\"Loss: {accum_loss}\")\n",
    "        accum_loss = 0\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab704f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,2,5)\n",
    "b = torch.rand(1,2,5,3)\n",
    "print(a)\n",
    "print(b)\n",
    "torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.array([0.8291, 0.5697, 0.3417, 0.6862, 0.0465]), np.array([0.2992, 0.0023, 0.4620, 0.0736, 0.9372]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0,0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7525a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,4,1,5)\n",
    "b = torch.rand(1,4,7,5)\n",
    "cos = nn.CosineSimilarity(dim=3, eps=1e-6)\n",
    "#for i in range(b.size()[2]):\n",
    "#    print(cos(a, b[:,:,i,:]))\n",
    "    \n",
    "a.expand(b.size())\n",
    "sim = torch.exp(cos(a,b) / 5)\n",
    "print(sim.size())\n",
    "print(sim[:,:,0].size())\n",
    "print(torch.sum(sim, dim=2).size())\n",
    "sim[:,:,0] / torch.sum(sim,dim=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
